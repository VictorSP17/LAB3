{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet Allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *J.*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Maxime Lucas Lanvin*\n",
    "* *Victor Salvia*\n",
    "* *Erik Axel Wilhelm Sjöberg*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 3 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/salvia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import json\n",
    "from operator import itemgetter\n",
    "\n",
    "# Import LibEx1\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams \n",
    "from utils import load_json, load_pkl\n",
    "import copy\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Import Data\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some data pre-processing done in Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs a term-document matrix from a corpus\n",
    "def get_term_document_matrix(corpus):\n",
    "    global_dictionary, dictionary_mapping = get_dictionary(corpus)\n",
    "    df_index = dict((k,len(list(set(v)))) for k,v in dictionary_mapping.items()) # A dict where the key is the term and the value is in how many documents the term is present\n",
    "    unique_words = list(df_index.keys()) #The unique words\n",
    "    word_to_index = dict(zip(unique_words,(range(len(unique_words))))) # Mapping from word to index (That we use for the encoding)\n",
    "    index_to_word = dict((v,k) for k,v in word_to_index.items()) # Mapping back from index to word. \n",
    "\n",
    "\n",
    "    m = len(unique_words)\n",
    "    n = len(corpus)\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    columns = []\n",
    "\n",
    "    for i in range(n):\n",
    "        tokens = corpus[i]['description']\n",
    "        loc_word_count = len(tokens)\n",
    "        loc_counts = Counter(tokens)\n",
    "        unique_tokens = list(loc_counts.keys())\n",
    "\n",
    "        for token in unique_tokens:\n",
    "            tf = loc_counts[token]/loc_word_count\n",
    "            df = df_index[token]\n",
    "            idf = np.log(n/(df+1))\n",
    "\n",
    "            rows.append(word_to_index[token])\n",
    "            columns.append(i)\n",
    "            values.append(tf*idf)\n",
    "\n",
    "    return csr_matrix((values, (rows, columns)), shape=(m, n)), index_to_word, word_to_index\n",
    "\n",
    "def get_column_scores(mat, i,index_to_word, n=-1):\n",
    "    a = mat.getcol(i)\n",
    "    non_zero_rows = csr_matrix.nonzero(a)[0]\n",
    "    d = {}\n",
    "    for i in non_zero_rows:\n",
    "        d[index_to_word[i]] = a[i,0]\n",
    "    order_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    if n == -1:\n",
    "        return Counter(order_d).most_common()\n",
    "    else:\n",
    "        return Counter(order_d).most_common(n)\n",
    "    \n",
    "def get_row_scores(mat, word,word_to_index, n=-1,corpus=courses):\n",
    "    a = mat.getrow(word_to_index[word])\n",
    "    non_zero_cols = csr_matrix.nonzero(a)[1]\n",
    "    d = {}\n",
    "    for i in non_zero_cols:\n",
    "        d[corpus[i]['name']] = a[0,i]\n",
    "    \n",
    "    order_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    if n == -1:\n",
    "        return Counter(order_d).most_common()\n",
    "    else:\n",
    "        return Counter(order_d).most_common(n)\n",
    "    \n",
    "# We do not care for the position here so just add them up\n",
    "def corpus_merge(c1,c2):\n",
    "    merged_corpus =  copy.deepcopy(c1)\n",
    "    for i in range(len(c1)):\n",
    "        merged_corpus[i]['description'] = merged_corpus[i]['description'] + c2[i]['description']\n",
    "    return merged_corpus\n",
    "\n",
    "# Checks whether or not there is a digit in a string.\n",
    "def NoNumbers(s):\n",
    "    return not any(char.isdigit() for char in s)\n",
    "\n",
    "# Stemms a given string\n",
    "def stemmer(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [ps.stem(w) for w in word_tokens if not w in ignored_words] \n",
    "    return [w for w in temp_list if NoNumbers(w)]\n",
    "\n",
    "# Lemmatizes a given string\n",
    "def lemmazation(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [lemmatizer.lemmatize(w) for w in word_tokens if not w in ignored_words]\n",
    "    return [w for w in temp_list if NoNumbers(w)]\n",
    "\n",
    "def lem_n_stem(s):\n",
    "    word_tokens = tknzr.tokenize(s)\n",
    "    temp_list = [ps.stem(lemmatizer.lemmatize(w)) for w in word_tokens if not w in ignored_words]\n",
    "    return [w for w in temp_list if NoNumbers(w)]    \n",
    "\n",
    "# Helper function for the tokenize_1gram.\n",
    "# tokenzie a given string, either stem or Lemmatise the words and removes the ignored words for a 1 gram\n",
    "def tokenize_1gram(l,lem,stemlem):\n",
    "    courses_loc = copy.deepcopy(l)\n",
    "    for i in courses_loc:\n",
    "        description = i['description']\n",
    "        if lem == True:\n",
    "            i['description'] = lemmazation(description)\n",
    "            if stemlem == False:\n",
    "                i['description'] = lemmazation(description)\n",
    "            else:\n",
    "                i['description'] = lem_n_stem(description)\n",
    "        else:\n",
    "            i['description'] = stemmer(description)  \n",
    "    return courses_loc\n",
    "\n",
    "# Description: Tokenzie a given string, either stem or Lemmatise the words and removes the ignored words for a 1 gram.\n",
    "# After this step n-grams are created over the cleaned string //\n",
    "\n",
    "# @ l: Indicats the level of the n-gram we want returned over the string l. Default is 1.\n",
    "# @ lem: boolean exression determining whether or not to use stemming or lemmazation. Default is lemmazation\n",
    "# @ stemlem: boolean expression determining whether or not to use both stemming and lemmazation\n",
    "def tokenize_ngram(l,n=1,lem=True,stemlem=False):\n",
    "    if n ==1:\n",
    "        return tokenize_1gram(l,lem,stemlem)  \n",
    "    courses_loc = copy.deepcopy(l)\n",
    "    for i in courses_loc:\n",
    "        description = i['description']\n",
    "        sentences = description.split('.')\n",
    "        grams = []\n",
    "        for s in sentences:\n",
    "            if lem == True:\n",
    "                if stemlem == False:\n",
    "                    tokens = lemmazation(s)\n",
    "                else:\n",
    "                    tokens = lem_n_stem(s)\n",
    "            else:\n",
    "                tokens = stemmer(s)\n",
    "            grams = grams + list(ngrams(tokens,n))\n",
    "        i['description'] = grams\n",
    "    return courses_loc   \n",
    "\n",
    "def get_dictionary(d):\n",
    "    global_dictionary = []\n",
    "    dictionary_mapping = {}\n",
    "    for i in range(0,len(d)):\n",
    "        temp_list = d[i]\n",
    "        global_dictionary = global_dictionary + temp_list['description']\n",
    "        for w in temp_list['description']:\n",
    "            if w in dictionary_mapping:\n",
    "                dictionary_mapping[w].append(i)\n",
    "            else:\n",
    "                dictionary_mapping[w] = [i]\n",
    "    return global_dictionary, dictionary_mapping\n",
    "\n",
    "\n",
    "\n",
    "# Creating a list containing all of the special chars. This list is then added to the stopwords and together they form\n",
    "# the ignored words. These words will be removed from the corpus. \n",
    "specialchar = ['.', ',', '(', ')', '&', ':', '/','-','\"',';','', ' ', '..', '...',\"'\",'%']\n",
    "ignored_words = set(list(stopwords) + specialchar)\n",
    "\n",
    "\n",
    "# Creating the tokenizer, the stemmer and the lemmatizer.\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True,preserve_case=False)\n",
    "ps = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "# Tokenzing, Lemmatizing & Stemming the corpus, for 1-grams, 2-grams and 3-grams\n",
    "lem_and_stem_1gram = tokenize_ngram(courses,1,stemlem=True)\n",
    "lem_and_stem_2gram = tokenize_ngram(courses,2,stemlem=True)\n",
    "\n",
    "lem_stem_corpus = corpus_merge(lem_and_stem_1gram, lem_and_stem_2gram)\n",
    "X, index_to_word, word_to_index = get_term_document_matrix(lem_stem_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lem_stem_corpus[10]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.8: Topics extraction\n",
    "\n",
    "Using your pre-processed courses dataset, extract topics using LDA.\n",
    "\n",
    "1. Print k = 10 topics extracted using LDA and give them labels.\n",
    "2. How does it compare with LSI?\n",
    "\n",
    "You can use the default values for all parameters.\n",
    "\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LDA with k = 10 and print the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors used for the documents will be the TF-IDF weighting. This is not strictly necessary but it can be helpful. We will select the 10.000 terms with highest TF-IDF for the sake of efficency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_f = csr_matrix.toarray(X)\n",
    "\n",
    "mX = np.zeros(87304)\n",
    "for i in range(87304):\n",
    "    mX[i] = np.max(X_f[i,:]) # maximum TF-IDF for each term -> it tells us importance of that term.\n",
    "\n",
    "indx_map = mX.argsort()[-10000:][::-1]\n",
    "X_f = X_f[indx_map,:]\n",
    "\n",
    "data = []\n",
    "for i in range(854):\n",
    "    data.append([i+1, Vectors.dense(X_f[:,i])])\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = LDA.train(rdd, k = 10, maxIterations = 20, optimizer = 'online', seed = 1)\n",
    "\n",
    "topics = model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the topics matrix, this function returns the distribution over the most frequent words in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def explain_topics(topics):\n",
    "    print(\"----------------- TOPIC 1 ---------------------\")\n",
    "    indx = topics[:,0].argsort()[-10:][::-1]\n",
    "    tot = np.sum(topics[:,0])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],0]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],0]/tot)\n",
    "    print(\"----------------- TOPIC 2 ---------------------\")\n",
    "    indx = topics[:,1].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,1])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],1]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],1]/tot)\n",
    "    print(\"----------------- TOPIC 3 ---------------------\")\n",
    "    indx = topics[:,2].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,2])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],2]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],2]/tot)\n",
    "    print(\"----------------- TOPIC 4 ---------------------\")\n",
    "    indx = topics[:,3].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,3])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],3]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],3]/tot)\n",
    "    print(\"----------------- TOPIC 5 ---------------------\")\n",
    "    indx = topics[:,4].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,4])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],4]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],4]/tot)\n",
    "    print(\"----------------- TOPIC 6 ---------------------\")\n",
    "    indx = topics[:,5].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,5])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],5]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],5]/tot)\n",
    "    print(\"----------------- TOPIC 7 ---------------------\")\n",
    "    indx = topics[:,6].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,6])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],6]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],6]/tot)\n",
    "    print(\"----------------- TOPIC 8 ---------------------\")\n",
    "    indx = topics[:,7].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,7])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],7]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],7]/tot)\n",
    "    print(\"----------------- TOPIC 9 ---------------------\")\n",
    "    indx = topics[:,8].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,8])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],8]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],8]/tot)\n",
    "    print(\"----------------- TOPIC 10 ---------------------\")\n",
    "    indx = topics[:,9].argsort()[-10:][::-1] \n",
    "    tot = np.sum(topics[:,9])\n",
    "    print(index_to_word[indx_map[indx[0]]], 1000*topics[indx[0],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[1]]], 1000*topics[indx[1],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[2]]], 1000*topics[indx[2],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[3]]], 1000*topics[indx[3],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[4]]], 1000*topics[indx[4],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[5]]], 1000*topics[indx[5],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[6]]], 1000*topics[indx[6],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[7]]], 1000*topics[indx[7],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[8]]], 1000*topics[indx[8],9]/tot)\n",
    "    print(index_to_word[indx_map[indx[9]]], 1000*topics[indx[9],9]/tot)\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- TOPIC 1 ---------------------\n",
      "('train', 'rotat') 0.35799698054722356\n",
      "rotat 0.2920716406155892\n",
      "train 0.25596290801729404\n",
      "instabl 0.15461347433814732\n",
      "('kinet', 'theori') 0.1432176009619948\n",
      "grade 0.1425379750910628\n",
      "statist 0.14251262568976553\n",
      "group 0.14224781064092867\n",
      "('knowledg', 'acquir') 0.14214217768274984\n",
      "mhd 0.1416211635746016\n",
      "----------------- TOPIC 2 ---------------------\n",
      "('design', 'xtal') 0.14209834601375732\n",
      "seminar 0.14064924771654994\n",
      "slope 0.14039854893608283\n",
      "('programm', 'student') 0.1390648347425649\n",
      "chemic 0.13896130630952525\n",
      "model 0.13890523291931878\n",
      "('seismic', 'design') 0.13857355001585206\n",
      "('identifi', 'variant') 0.1372152157585887\n",
      "('project', 'hum') 0.13660893887533485\n",
      "main 0.1356469386963386\n",
      "----------------- TOPIC 3 ---------------------\n",
      "energi 0.16223567868571018\n",
      "pile 0.15164160390621703\n",
      "geostructur 0.15025245121169167\n",
      "('european', 'intern') 0.14393046174030732\n",
      "issu 0.14279224268749263\n",
      "('rule', 'principl') 0.14260015661584985\n",
      "swiss 0.14255109777423947\n",
      "('main', 'contractu') 0.1418909058243941\n",
      "('examin', 'main') 0.14124846087362944\n",
      "('energi', 'law') 0.14081721200206718\n",
      "----------------- TOPIC 4 ---------------------\n",
      "('administr', 'enrol') 0.22081526194769846\n",
      "administr 0.21572534026737597\n",
      "edmt 0.20171431586681074\n",
      "('edmt', 'administr') 0.20134771378596014\n",
      "enrol 0.1994977248642852\n",
      "snow 0.19451141549071088\n",
      "('contact', 'edmt') 0.19137705307300765\n",
      "program 0.16023895397084528\n",
      "heat 0.15652528874728427\n",
      "contact 0.1552082297271577\n",
      "----------------- TOPIC 5 ---------------------\n",
      "risk 0.17370463676655745\n",
      "statement 0.17211709625766847\n",
      "financi 0.16733697967061612\n",
      "('financi', 'statement') 0.1645253956564294\n",
      "account 0.15177993310740404\n",
      "('theori', 'metric') 0.14759889421827246\n",
      "busi 0.14709094410928383\n",
      "('numer', 'method') 0.14695623189419005\n",
      "system 0.14636265311275606\n",
      "metric 0.14562986944219475\n",
      "----------------- TOPIC 6 ---------------------\n",
      "('natur', 'evolut') 0.1411504894378187\n",
      "cmo 0.13678853642761926\n",
      "('elementari', 'transistor') 0.1346717911567661\n",
      "toxic 0.13454639644435995\n",
      "audio 0.1345193092236134\n",
      "('research', 'project') 0.1339783305494208\n",
      "('discu', 'fate') 0.1329035340680558\n",
      "('model', 'assess') 0.1328966380456924\n",
      "facad 0.13240049053416153\n",
      "technolog 0.13222758332982212\n",
      "----------------- TOPIC 7 ---------------------\n",
      "('project', 'ic') 0.17388024143208247\n",
      "('ic', 'laboratori') 0.16481372543839556\n",
      "devic 0.1470402744733337\n",
      "ic 0.14642963116686636\n",
      "('geometr', 'analysi') 0.14585328481431348\n",
      "('suppli', 'chain') 0.1456764361224746\n",
      "mechan 0.14539132235185134\n",
      "sensor 0.14399350334974945\n",
      "probabl 0.1436756015812147\n",
      "('semest', 'project') 0.14302378930004273\n",
      "----------------- TOPIC 8 ---------------------\n",
      "lie 0.15046135694485901\n",
      "digit 0.14036258885208044\n",
      "('group', 'lie') 0.14015246763856343\n",
      "displac 0.14003821176698814\n",
      "structur 0.13877598962862003\n",
      "('circuitsinterpret', 'transistor-level') 0.13871442414282406\n",
      "ethic 0.1386846647647045\n",
      "('background', 'synthet') 0.13715915417837607\n",
      "ee 0.13599773323988135\n",
      "('encount', 'mathemat') 0.13574831020580616\n",
      "----------------- TOPIC 9 ---------------------\n",
      "('drug', '?') 0.17612904664981702\n",
      "('fondament', 'question') 0.17549072088389903\n",
      "('compound', 'drug') 0.17015756615997468\n",
      "drug 0.1676039972532851\n",
      "('provid', 'fondament') 0.16464453968254197\n",
      "('high', 'energi') 0.16320606420035627\n",
      "('chemic', 'compound') 0.16070400066949408\n",
      "('question', 'chemic') 0.1583888457690001\n",
      "fondament 0.1522932843077809\n",
      "epistemolog 0.14837181958648932\n",
      "----------------- TOPIC 10 ---------------------\n",
      "project 0.15376579396284307\n",
      "hous 0.1499026172065141\n",
      "process 0.14828297566298904\n",
      "('fundament', 'optic') 0.14564215966874985\n",
      "curv 0.14508987741971927\n",
      "develop 0.1441404407427917\n",
      "biomimet 0.14040501213802534\n",
      "('matter', 'simpl') 0.13912658102226938\n",
      "('week', 'suppos') 0.13899977035446415\n",
      "('mechan', 'stellar') 0.13821587707021132\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "topics = model.topicsMatrix()\n",
    "explain_topics(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels for each topic?\n",
    "\n",
    "1. Training rotation program.\n",
    "2. -.\n",
    "3. Energy and policy.\n",
    "4. EDMT management program.\n",
    "5. Business, risk, finances.\n",
    "6. -.\n",
    "7. Projects in IC department: supply chain, sensors, probability, ...\n",
    "8. Lie groups, etc.\n",
    "9. Drugs, compunds, energy, chemistry.\n",
    "10. -."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it compare with LSI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is that all the topics are much more interpretable than in the LSI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the effects of α (distribution of topics in documents) and β (distribution of words in topics). You should start by reading the documentation of pyspark.mllib.clustering.LDA.\n",
    "\n",
    "1. Fix k = 10 and β = 1.01, and vary α. How does it impact the topics?\n",
    "2. Fix k = 10 and α = 6, and vary β. How does it impact the topics?\n",
    "\n",
    "Hint: You can set the seed to produce more comparable outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* docConcentration (alpha) = -1, topicConcentration (beta) = -1, are the defoult settings in the LDA training functio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Fix k = 10 and Beta = 1.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in $\\alpha$ means the following: higher values of $\\alpha$ mean that documents have more topics associated, and lower values of it mean that a document contains less topics. A priori we would then like it to be small because each course (document) is pretty specific of an area, it could belong to 2, 3 or 4 main topics at most. Otherwise if we allow documents to belong to many topics it might mean that the topics are very broad and not specific.\n",
    "\n",
    "* In conclusion, the effect we should see is that if $\\alpha$ is high, topics will be very broad. And if it is low, topics should be much more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\beta$ = 1.01, $\\alpha$ = 0.01.\n",
    "2. $\\beta$ = 1.01, $\\alpha$ = 10.\n",
    "\n",
    "We should be able to see that in 1 the topics are more specific than in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- TOPIC 1 ---------------------\n",
      "('train', 'rotat') 0.2547868432436578\n",
      "rotat 0.21525036703247685\n",
      "train 0.19518654784311673\n",
      "('contact', 'edmt') 0.16709343422409062\n",
      "('administr', 'enrol') 0.16670002716896526\n",
      "edmt 0.16520108807045475\n",
      "('edmt', 'administr') 0.1607289256930936\n",
      "administr 0.1549359516667174\n",
      "enrol 0.15081110249294236\n",
      "contact 0.13768764107667156\n",
      "----------------- TOPIC 2 ---------------------\n",
      "model 0.13196082602363426\n",
      "financi 0.1267805663382578\n",
      "optic 0.12591094369009206\n",
      "('design', 'xtal') 0.12507996327059198\n",
      "robot 0.12268654834662665\n",
      "radiat 0.12238021390233196\n",
      "('identifi', 'variant') 0.12215901199745008\n",
      "implant 0.12186112939836594\n",
      "('project', 'hum') 0.12179633940156404\n",
      "main 0.12179484314907998\n",
      "----------------- TOPIC 3 ---------------------\n",
      "energi 0.13693368191883776\n",
      "test 0.12742616572960677\n",
      "pile 0.12532208251053942\n",
      "geostructur 0.12515305814754338\n",
      "rate 0.12505052467343541\n",
      "('dimens', 'stress') 0.12418184620007201\n",
      "issu 0.12413568698174024\n",
      "('fundament', 'concept') 0.12344726876316109\n",
      "('respect', 'histor') 0.1215034369400813\n",
      "('word', 'map') 0.12124429325338741\n",
      "----------------- TOPIC 4 ---------------------\n",
      "energi 0.12877225566408387\n",
      "wireless 0.1260587843133763\n",
      "('energi', 'balanc') 0.12422500192550544\n",
      "emphasi 0.12418187339301819\n",
      "simul 0.12294812485665575\n",
      "program 0.12291870732780542\n",
      "discontinu 0.12258884536407956\n",
      "chemic 0.1224314159159754\n",
      "broad 0.12160451057499082\n",
      "('mode', 'vacuum') 0.12096797313826362\n",
      "----------------- TOPIC 5 ---------------------\n",
      "('electron', 'microscopi') 0.13161876768196784\n",
      "('project', 'ic') 0.13130205901202413\n",
      "('ic', 'laboratori') 0.1307729404503\n",
      "('semest', 'project') 0.13065064369181398\n",
      "ic 0.12875180317821522\n",
      "('integr', 'transform') 0.12448343022583994\n",
      "risk 0.12333707040462302\n",
      "quantum 0.1225944963333368\n",
      "hous 0.12250462577810954\n",
      "('numer', 'method') 0.12222561745789233\n",
      "----------------- TOPIC 6 ---------------------\n",
      "biomass 0.12845566900320796\n",
      "reaction 0.12504280775992677\n",
      "statist 0.12307754482201778\n",
      "('natur', 'evolut') 0.1226129663810552\n",
      "edbb 0.12231334776887996\n",
      "('discu', 'fate') 0.1222435181077269\n",
      "('homotopi', 'theori') 0.12171361638168685\n",
      "systemat 0.12095101962379731\n",
      "toxic 0.12081945451093408\n",
      "leukemia 0.12060323431126119\n",
      "----------------- TOPIC 7 ---------------------\n",
      "mechan 0.13004603766733186\n",
      "('chapter', 'total') 0.12504779056719326\n",
      "week 0.12354685996266428\n",
      "electron 0.12311156303191963\n",
      "('=', 'de') 0.12218386199907189\n",
      "thin 0.1217293493912499\n",
      "biochem 0.12158305409736361\n",
      "devic 0.12141860115320752\n",
      "film 0.12107830096484425\n",
      "('model', 'optim') 0.11996043893241334\n",
      "----------------- TOPIC 8 ---------------------\n",
      "laser 0.14625484096659727\n",
      "snow 0.14128053027157636\n",
      "flow 0.12879444405949395\n",
      "metric 0.12834309013002565\n",
      "food 0.12707117877453614\n",
      "cytometri 0.12384866323221885\n",
      "displac 0.12375753941494447\n",
      "static 0.12350806508957128\n",
      "hematopoiet 0.1233783807533454\n",
      "pump 0.12252296551977923\n",
      "----------------- TOPIC 9 ---------------------\n",
      "('drug', '?') 0.14555234050544594\n",
      "('fondament', 'question') 0.14517014786495697\n",
      "('compound', 'drug') 0.14197634698650197\n",
      "drug 0.14061643703358945\n",
      "('provid', 'fondament') 0.13867495892612977\n",
      "('chemic', 'compound') 0.13631484489556717\n",
      "('question', 'chemic') 0.1349288724364611\n",
      "fondament 0.13127800084938784\n",
      "design 0.12663671468324722\n",
      "control 0.12646379275996966\n",
      "----------------- TOPIC 10 ---------------------\n",
      "process 0.12751191544957388\n",
      "('fundament', 'optic') 0.12458162897062335\n",
      "biomimet 0.12451092456295199\n",
      "develop 0.12430193378437397\n",
      "('week', 'suppos') 0.12366611604650374\n",
      "('mechan', 'stellar') 0.12319508423879084\n",
      "('dividend', 'hedg') 0.12264255787451643\n",
      "project 0.12263307176530663\n",
      "real 0.12256621977918555\n",
      "('teach', 'lab') 0.12207296663465993\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_f = csr_matrix.toarray(X)\n",
    "\n",
    "mX = np.zeros(87304)\n",
    "for i in range(87304):\n",
    "    mX[i] = np.max(X_f[i,:]) # maximum TF-IDF for each term -> it tells us importance of that term.\n",
    "\n",
    "indx_map = mX.argsort()[-10000:][::-1]\n",
    "X_f = X_f[indx_map,:]\n",
    "\n",
    "data = []\n",
    "for i in range(854):\n",
    "    data.append([i+1, Vectors.dense(X_f[:,i])])\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = LDA.train(rdd, k = 10, maxIterations = 20, optimizer = 'online', topicConcentration = 1.01, docConcentration = 0.01, seed = 1)\n",
    "\n",
    "topics = model.topicsMatrix()\n",
    "explain_topics(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- TOPIC 1 ---------------------\n",
      "('train', 'rotat') 0.1294358244768289\n",
      "statist 0.1251948419011645\n",
      "('radtk', 'laboratori') 0.1225773738454969\n",
      "fractur 0.12234101225392062\n",
      "group 0.12231448001679913\n",
      "design 0.12231230404945463\n",
      "('featur', 'limit') 0.1222742960447491\n",
      "grade 0.12213863784827582\n",
      "inerti 0.12145349872634861\n",
      "('target-bas', 'screen') 0.12101555953643758\n",
      "----------------- TOPIC 2 ---------------------\n",
      "model 0.1270624082727119\n",
      "('design', 'xtal') 0.12588623196521995\n",
      "main 0.1231411471428971\n",
      "('identifi', 'variant') 0.12250949807452988\n",
      "('project', 'hum') 0.122366382380494\n",
      "seminar 0.12215264651624859\n",
      "energi 0.12183286777278854\n",
      "('problem', 'student') 0.12170364704670543\n",
      "extern 0.12126807190833111\n",
      "formul 0.12124258802235634\n",
      "----------------- TOPIC 3 ---------------------\n",
      "('fundament', 'concept') 0.1240936643406208\n",
      "programm 0.12327393614522353\n",
      "('dimens', 'stress') 0.12268407597528767\n",
      "('train', 'rotat') 0.12187326492695047\n",
      "topic 0.12040287396644356\n",
      "('hidden', 'markov') 0.12036956695652465\n",
      "comput 0.12021585943387803\n",
      "('financ', 'dividend') 0.12019383804676495\n",
      "('properti', 'polym') 0.12004222595759174\n",
      "overview 0.12001775256816959\n",
      "----------------- TOPIC 4 ---------------------\n",
      "energi 0.12841993639581956\n",
      "emphasi 0.1264556942010412\n",
      "program 0.12578951553772505\n",
      "discontinu 0.12307383213098214\n",
      "('energi', 'balanc') 0.12271549729178531\n",
      "broad 0.12263495066294021\n",
      "chemic 0.12256999954605223\n",
      "('mode', 'vacuum') 0.1212885533660287\n",
      "('control', 'techniqu') 0.12066434655180418\n",
      "multiple-beam 0.12038572102648035\n",
      "----------------- TOPIC 5 ---------------------\n",
      "system 0.1252485800837029\n",
      "design 0.12469722699526929\n",
      "('integr', 'transform') 0.12452059466455156\n",
      "quantum 0.12219554534379615\n",
      "('point', 'mass') 0.12217323987178201\n",
      "('label', 'imag') 0.12208224526629344\n",
      "('electron', 'microscopi') 0.12160027709108656\n",
      "('practic', 'implement') 0.12062945316572316\n",
      "('field', 'microscopi') 0.12033240109364608\n",
      "('numer', 'method') 0.12022834518359023\n",
      "----------------- TOPIC 6 ---------------------\n",
      "train 0.12419727265470537\n",
      "('natur', 'evolut') 0.12293696937362496\n",
      "toxic 0.12138969373726478\n",
      "technolog 0.12138234374843157\n",
      "optim 0.12109843105561661\n",
      "molecular 0.1205520283283412\n",
      "('elementari', 'transistor') 0.12047527060788712\n",
      "('discu', 'fate') 0.12034030136292179\n",
      "('model', 'assess') 0.12027436968150318\n",
      "('elementari', 'composit') 0.11982376363819343\n",
      "----------------- TOPIC 7 ---------------------\n",
      "mechan 0.12538161715239024\n",
      "biochem 0.12288813715214986\n",
      "('=', 'de') 0.12248590270324113\n",
      "('model', 'optim') 0.12044686281074972\n",
      "final 0.12029731800136727\n",
      "research 0.11972539559071543\n",
      "week 0.1195513415842828\n",
      "physiqu 0.11954177370348014\n",
      "('ture', 'degre') 0.11948666483627103\n",
      "('chapter', 'total') 0.11923821127774546\n",
      "----------------- TOPIC 8 ---------------------\n",
      "displac 0.12515849719511776\n",
      "ethic 0.12223304537125104\n",
      "('background', 'synthet') 0.1220482586790116\n",
      "final 0.12157962686101396\n",
      "studi 0.12086505297959303\n",
      "('main', 'legal') 0.12052927181378925\n",
      "school 0.12004443804398113\n",
      "structur 0.12004207048063678\n",
      "('encount', 'mathemat') 0.11927419449307755\n",
      "expertis 0.11924108376967288\n",
      "----------------- TOPIC 9 ---------------------\n",
      "servic 0.1261500108032506\n",
      "epistemolog 0.12576263091476592\n",
      "control 0.12369937548689718\n",
      "('diffract', 'advis') 0.12236096956714367\n",
      "('creep', 'shrinkag') 0.1219162281750695\n",
      "('project', 'hum') 0.12097578323383144\n",
      "exercis 0.12092781741799025\n",
      "('lung', 'tumor') 0.12015999743157074\n",
      "('posit', 'signal') 0.12011499544104443\n",
      "('introductori', 'tem') 0.11993526102902373\n",
      "----------------- TOPIC 10 ---------------------\n",
      "project 0.13050257333396673\n",
      "process 0.12877532225144794\n",
      "('fundament', 'optic') 0.12558863887295968\n",
      "biomimet 0.12469979835649062\n",
      "('week', 'suppos') 0.12420562441054393\n",
      "develop 0.1237822460746879\n",
      "('mechan', 'stellar') 0.12332864884520689\n",
      "('dividend', 'hedg') 0.12281711664597102\n",
      "optim 0.12159604874136266\n",
      "('teach', 'lab') 0.12073006668262082\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_f = csr_matrix.toarray(X)\n",
    "\n",
    "mX = np.zeros(87304)\n",
    "for i in range(87304):\n",
    "    mX[i] = np.max(X_f[i,:]) # maximum TF-IDF for each term -> it tells us importance of that term.\n",
    "\n",
    "indx_map = mX.argsort()[-10000:][::-1]\n",
    "X_f = X_f[indx_map,:]\n",
    "\n",
    "data = []\n",
    "for i in range(854):\n",
    "    data.append([i+1, Vectors.dense(X_f[:,i])])\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = LDA.train(rdd, k = 10, maxIterations = 20, optimizer = 'online', docConcentration = 10.00, topicConcentration = 1.01, seed = 1)\n",
    "\n",
    "topics = model.topicsMatrix()\n",
    "explain_topics(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that in the first case topics are more specifip than in the second. This can be seen because the distribution of terms per topic is almost uniform in the case $\\alpha = 10$, whereas in $\\alpha = 0.01$ we can see there is more variability and each topic tends to have more predominant terms.\n",
    "\n",
    "In our case we want to have topics that can be interpreted, hence we will choose $\\alpha = 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Fix k = 10 and Alpha = 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\beta$ = 0.01, $\\alpha$ = 6.\n",
    "2. $\\beta$ = 10, $\\alpha$ = 6.\n",
    "\n",
    "As in the other case before, now we expect that higher values of $\\beta$ mean that topics have more words, wherear lower values mean that topics are more specific (fewer words). We should be able to see that in 1 there are fewer words per topic than in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- TOPIC 1 ---------------------\n",
      "('train', 'rotat') 0.15490682362656905\n",
      "statist 0.1482830827955895\n",
      "group 0.14282911790157543\n",
      "design 0.14217753722158835\n",
      "fractur 0.14148524474541388\n",
      "grade 0.14104393124453196\n",
      "('radtk', 'laboratori') 0.14070906335680494\n",
      "('featur', 'limit') 0.1398243415030838\n",
      "inerti 0.13954291378429842\n",
      "analysi 0.13843573295263809\n",
      "----------------- TOPIC 2 ---------------------\n",
      "model 0.1514697734769687\n",
      "('design', 'xtal') 0.14658206651706837\n",
      "main 0.14412083297238557\n",
      "seminar 0.14172837515907555\n",
      "energi 0.14137199151781765\n",
      "('identifi', 'variant') 0.14045516501789218\n",
      "('project', 'hum') 0.14022002167104655\n",
      "chemic 0.13909528034681787\n",
      "extern 0.13903151370955244\n",
      "('problem', 'student') 0.13889951663300718\n",
      "----------------- TOPIC 3 ---------------------\n",
      "('fundament', 'concept') 0.1439661554754502\n",
      "programm 0.14218903073977193\n",
      "('dimens', 'stress') 0.1405724659592668\n",
      "('train', 'rotat') 0.14003365837630677\n",
      "topic 0.13844099743819716\n",
      "comput 0.13758540930344307\n",
      "overview 0.1366378596601052\n",
      "('hidden', 'markov') 0.1365316615581298\n",
      "('financ', 'dividend') 0.13613933172809686\n",
      "advanc 0.13613305520117824\n",
      "----------------- TOPIC 4 ---------------------\n",
      "energi 0.15468657547784914\n",
      "program 0.14944686140347976\n",
      "emphasi 0.1483215810506317\n",
      "chemic 0.14239510438528788\n",
      "discontinu 0.14147658812865044\n",
      "broad 0.14120208305423823\n",
      "('energi', 'balanc') 0.14096797442519185\n",
      "('mode', 'vacuum') 0.13819229139036274\n",
      "('control', 'techniqu') 0.1369463299427607\n",
      "multiple-beam 0.13655459821914134\n",
      "----------------- TOPIC 5 ---------------------\n",
      "system 0.147937441972435\n",
      "design 0.14704196615637577\n",
      "('integr', 'transform') 0.14384013088831377\n",
      "quantum 0.1418047574577626\n",
      "('label', 'imag') 0.13987359418120537\n",
      "('point', 'mass') 0.1396629490879399\n",
      "('electron', 'microscopi') 0.13931110608893957\n",
      "('practic', 'implement') 0.1370166159633345\n",
      "('numer', 'method') 0.13697171321500573\n",
      "model 0.1366523448005558\n",
      "----------------- TOPIC 6 ---------------------\n",
      "train 0.14465966595598523\n",
      "('natur', 'evolut') 0.14107033051207804\n",
      "technolog 0.1397829960019784\n",
      "optim 0.1389919566621134\n",
      "toxic 0.13851563431076216\n",
      "molecular 0.1381912407018979\n",
      "('elementari', 'transistor') 0.1366113333861106\n",
      "('model', 'assess') 0.13658479058314624\n",
      "('discu', 'fate') 0.13656984671900588\n",
      "('elementari', 'composit') 0.135699855272514\n",
      "----------------- TOPIC 7 ---------------------\n",
      "mechan 0.14751028750956577\n",
      "biochem 0.14116872235200315\n",
      "('=', 'de') 0.14028089874988856\n",
      "research 0.137758614401862\n",
      "final 0.1374703086922657\n",
      "('model', 'optim') 0.13670473794121066\n",
      "week 0.1362320875515378\n",
      "system 0.1356060696840753\n",
      "devic 0.1354714905392632\n",
      "('basic', 'notion') 0.13511274556048888\n",
      "----------------- TOPIC 8 ---------------------\n",
      "displac 0.14533958392655383\n",
      "ethic 0.1403486445305756\n",
      "final 0.1398948221408999\n",
      "('background', 'synthet') 0.13940076000941057\n",
      "studi 0.13933342496230625\n",
      "('main', 'legal') 0.1379227179208398\n",
      "structur 0.1376383213073609\n",
      "school 0.13654078317541493\n",
      "statist 0.1360090423201851\n",
      "flow 0.13540536322008892\n",
      "----------------- TOPIC 9 ---------------------\n",
      "servic 0.1479197441828093\n",
      "epistemolog 0.14650654029408175\n",
      "control 0.14412479156542565\n",
      "('diffract', 'advis') 0.13995450984297442\n",
      "exercis 0.13950190700284043\n",
      "('creep', 'shrinkag') 0.13915982464908463\n",
      "('project', 'hum') 0.13770433065010462\n",
      "('lung', 'tumor') 0.13602170754601153\n",
      "('posit', 'signal') 0.1359412958366359\n",
      "('introductori', 'tem') 0.13562013779150944\n",
      "----------------- TOPIC 10 ---------------------\n",
      "project 0.15848534482024615\n",
      "process 0.15437341387882772\n",
      "('fundament', 'optic') 0.14621145171664623\n",
      "develop 0.14488644331441095\n",
      "biomimet 0.1442946910263983\n",
      "('week', 'suppos') 0.14342811231902644\n",
      "('mechan', 'stellar') 0.14167901162423643\n",
      "('dividend', 'hedg') 0.14080570036388962\n",
      "optim 0.13992066043955012\n",
      "hands-on 0.13775263739036617\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_f = csr_matrix.toarray(X)\n",
    "\n",
    "mX = np.zeros(87304)\n",
    "for i in range(87304):\n",
    "    mX[i] = np.max(X_f[i,:]) # maximum TF-IDF for each term -> it tells us importance of that term.\n",
    "\n",
    "indx_map = mX.argsort()[-10000:][::-1]\n",
    "X_f = X_f[indx_map,:]\n",
    "\n",
    "data = []\n",
    "for i in range(854):\n",
    "    data.append([i+1, Vectors.dense(X_f[:,i])])\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = LDA.train(rdd, k = 10, maxIterations = 20, optimizer = 'online', docConcentration = 6.0, topicConcentration = 0.01, seed = 1)\n",
    "\n",
    "topics = model.topicsMatrix()\n",
    "explain_topics(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- TOPIC 1 ---------------------\n",
      "('train', 'rotat') 0.1298313919676481\n",
      "statist 0.12544076427291817\n",
      "('radtk', 'laboratori') 0.12278080406256044\n",
      "fractur 0.12255043584661819\n",
      "group 0.1225331334271251\n",
      "design 0.12252685133570017\n",
      "('featur', 'limit') 0.12247230619477349\n",
      "grade 0.12234538061021526\n",
      "inerti 0.12165121706475628\n",
      "('target-bas', 'screen') 0.12120572329390299\n",
      "----------------- TOPIC 2 ---------------------\n",
      "model 0.12732396979881322\n",
      "('design', 'xtal') 0.12611855475441858\n",
      "main 0.12336527616605018\n",
      "('identifi', 'variant') 0.1227108160628663\n",
      "('project', 'hum') 0.12256650494322932\n",
      "seminar 0.12236333607634961\n",
      "energi 0.12204482739894122\n",
      "('problem', 'student') 0.12189781015840838\n",
      "extern 0.12146374749729877\n",
      "formul 0.12143693735114733\n",
      "----------------- TOPIC 3 ---------------------\n",
      "('fundament', 'concept') 0.12431399606948602\n",
      "programm 0.12348507347288629\n",
      "('dimens', 'stress') 0.12288601348111584\n",
      "('train', 'rotat') 0.12207498678169265\n",
      "topic 0.1205970178599374\n",
      "('hidden', 'markov') 0.12055136611870836\n",
      "comput 0.12040580137264106\n",
      "('financ', 'dividend') 0.1203739404919321\n",
      "('properti', 'polym') 0.12022046205054736\n",
      "overview 0.12020181700731157\n",
      "----------------- TOPIC 4 ---------------------\n",
      "energi 0.12870250570688213\n",
      "emphasi 0.12669827407847037\n",
      "program 0.12604051848348777\n",
      "discontinu 0.12328033625410605\n",
      "('energi', 'balanc') 0.12292012335619612\n",
      "broad 0.12284268947626675\n",
      "chemic 0.12278492081787544\n",
      "('mode', 'vacuum') 0.12147849468241807\n",
      "('control', 'techniqu') 0.12084813010922643\n",
      "multiple-beam 0.12056764803549658\n",
      "----------------- TOPIC 5 ---------------------\n",
      "system 0.12549266886924465\n",
      "design 0.12493573435138307\n",
      "('integr', 'transform') 0.1247386326720001\n",
      "quantum 0.12240707106664327\n",
      "('point', 'mass') 0.12237061242264978\n",
      "('label', 'imag') 0.12228082493433202\n",
      "('electron', 'microscopi') 0.12179786366464816\n",
      "('practic', 'implement') 0.12081405242221398\n",
      "('field', 'microscopi') 0.1205131981527945\n",
      "('numer', 'method') 0.12041425670524639\n",
      "----------------- TOPIC 6 ---------------------\n",
      "train 0.1244665128504401\n",
      "('natur', 'evolut') 0.12314155672398584\n",
      "technolog 0.12158359827341315\n",
      "toxic 0.12158182213459685\n",
      "optim 0.12129472099694825\n",
      "molecular 0.12074466099587396\n",
      "('elementari', 'transistor') 0.12065739416230015\n",
      "('discu', 'fate') 0.12052382308502982\n",
      "('model', 'assess') 0.12045639085422251\n",
      "('elementari', 'composit') 0.12000179599614151\n",
      "----------------- TOPIC 7 ---------------------\n",
      "mechan 0.1256222572787014\n",
      "biochem 0.12309397296585349\n",
      "('=', 'de') 0.12268614478165836\n",
      "('model', 'optim') 0.12062983007147657\n",
      "final 0.12048578860439141\n",
      "research 0.11991717009009284\n",
      "week 0.1197340640276098\n",
      "physiqu 0.11971632758872233\n",
      "('ture', 'degre') 0.11966098397431606\n",
      "('chapter', 'total') 0.11941250865312443\n",
      "----------------- TOPIC 8 ---------------------\n",
      "displac 0.125384817415744\n",
      "ethic 0.12243456070764458\n",
      "('background', 'synthet') 0.12224420408308452\n",
      "final 0.12178017840429767\n",
      "studi 0.12106378754115797\n",
      "('main', 'legal') 0.1207164027797068\n",
      "structur 0.12023237407119693\n",
      "school 0.12022727726834266\n",
      "('encount', 'mathemat') 0.1194463638998543\n",
      "expertis 0.11941438210850863\n",
      "----------------- TOPIC 9 ---------------------\n",
      "servic 0.12639031543955187\n",
      "epistemolog 0.12599410022840635\n",
      "control 0.12392164940637437\n",
      "('diffract', 'advis') 0.12255962922531398\n",
      "('creep', 'shrinkag') 0.12211093619268497\n",
      "('project', 'hum') 0.12116327694187251\n",
      "exercis 0.121127947704086\n",
      "('lung', 'tumor') 0.12033910089167643\n",
      "('posit', 'signal') 0.12029369904700124\n",
      "('introductori', 'tem') 0.12011236764894703\n",
      "----------------- TOPIC 10 ---------------------\n",
      "project 0.13080236515877325\n",
      "process 0.12905096159960966\n",
      "('fundament', 'optic') 0.12581881071821369\n",
      "biomimet 0.12492025727585228\n",
      "('week', 'suppos') 0.1244217285502675\n",
      "develop 0.12401052024623417\n",
      "('mechan', 'stellar') 0.12353588876366625\n",
      "('dividend', 'hedg') 0.12301979393916186\n",
      "optim 0.12179720293194313\n",
      "('teach', 'lab') 0.12091533586253331\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_f = csr_matrix.toarray(X)\n",
    "\n",
    "mX = np.zeros(87304)\n",
    "for i in range(87304):\n",
    "    mX[i] = np.max(X_f[i,:]) # maximum TF-IDF for each term -> it tells us importance of that term.\n",
    "\n",
    "indx_map = mX.argsort()[-10000:][::-1]\n",
    "X_f = X_f[indx_map,:]\n",
    "\n",
    "data = []\n",
    "for i in range(854):\n",
    "    data.append([i+1, Vectors.dense(X_f[:,i])])\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = LDA.train(rdd, k = 10, maxIterations = 20, optimizer = 'online', docConcentration = 6.0, topicConcentration = 0.99, seed = 1)\n",
    "\n",
    "topics = model.topicsMatrix()\n",
    "explain_topics(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly in the former case in which $\\beta = 0.01$ the distribution per topic is more specific as we were expecting. For the sake of topic interpretability we will then choose $\\beta = 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.10: EPFL's taught subjects\n",
    "\n",
    "List the subjects of EPFL’s classes.\n",
    "\n",
    "1. Find the combination of k, α and β that gives the most interpretable topics.\n",
    "2. Explain why you chose these values.\n",
    "3. Report the values of the hyperparameters that you used and your labels for the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best combination for k, $\\alpha$, and $\\beta$ for topic interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have already decided to use low values of both $\\alpha$ and $\\beta$. Now is time to explore the $k$ hyperparameter. Intuitively from what we have seen before I would reduce it to 5 in order to have a much clear view of what each topic entails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- TOPIC 1 ---------------------\n",
      "('train', 'rotat') 0.29473832810867207\n",
      "rotat 0.24000170846046717\n",
      "train 0.21729319482789855\n",
      "hous 0.18139957266269213\n",
      "('simpl', 'complex') 0.1481690953202155\n",
      "('radtk', 'laboratori') 0.1438116903030069\n",
      "studio 0.14231303400418446\n",
      "('arriv', 'simpl') 0.14152303162110397\n",
      "instabl 0.14099049604012331\n",
      "('featur', 'limit') 0.13995137191816512\n",
      "----------------- TOPIC 2 ---------------------\n",
      "('problem', 'student') 0.1516067149238453\n",
      "organ 0.1470065477501906\n",
      "('interferometri', 'tv') 0.1467938246850633\n",
      "('design', 'xtal') 0.14486191316831973\n",
      "main 0.1425549556749034\n",
      "('programm', 'student') 0.1412210266353774\n",
      "slope 0.1398016569005256\n",
      "formul 0.13977501840523437\n",
      "('identifi', 'variant') 0.1396404964729827\n",
      "('project', 'hum') 0.13899219235215984\n",
      "----------------- TOPIC 3 ---------------------\n",
      "lifecycl 0.14566338309862087\n",
      "magnet 0.14218542749047636\n",
      "('dimens', 'stress') 0.1401290984664936\n",
      "programm 0.13989555776925003\n",
      "('fundament', 'concept') 0.13902390012543622\n",
      "geostructur 0.13710231212311713\n",
      "photoelectrochem 0.13665636123371833\n",
      "pile 0.1362375576479621\n",
      "topic 0.1361253231854879\n",
      "('hidden', 'markov') 0.13595535205173453\n",
      "----------------- TOPIC 4 ---------------------\n",
      "program 0.18845360439116085\n",
      "network 0.16100812840183176\n",
      "food 0.16058951715647451\n",
      "energi 0.1569701028804603\n",
      "optic 0.1524230705509723\n",
      "('stem', 'cell') 0.1487143612397461\n",
      "emphasi 0.14835205420312117\n",
      "('mobil', 'robot') 0.1450873638243169\n",
      "data 0.14430948654282252\n",
      "problem 0.142324960943693\n",
      "----------------- TOPIC 5 ---------------------\n",
      "('contact', 'edmt') 0.2515258931706188\n",
      "('edmt', 'administr') 0.2384317645683818\n",
      "edmt 0.23037965534373014\n",
      "('administr', 'enrol') 0.22659662087362614\n",
      "administr 0.2165213336658582\n",
      "enrol 0.20662496527851754\n",
      "contact 0.17555584536187074\n",
      "('numer', 'method') 0.14759481603292046\n",
      "forc 0.14553898798955156\n",
      "quantum 0.14486629632550924\n",
      "----------------- TOPIC 6 ---------------------\n",
      "biomass 0.17004978382602784\n",
      "optic 0.16009671626617555\n",
      "cell 0.15834143131430403\n",
      "membran 0.1516097229652687\n",
      "molecular 0.15139760495549498\n",
      "('doctor', 'student') 0.1511191356022947\n",
      "('biomass', 'convers') 0.1487058276706011\n",
      "prioriti 0.14713494419550657\n",
      "speech 0.14412178280254354\n",
      "doctor 0.1422664015857843\n",
      "----------------- TOPIC 7 ---------------------\n",
      "('project', 'ic') 0.23370789498305966\n",
      "('ic', 'laboratori') 0.22400447918809432\n",
      "('semest', 'project') 0.19415433552713082\n",
      "ic 0.19354149087127864\n",
      "('suppli', 'chain') 0.16421673868274092\n",
      "electron 0.16415797937287332\n",
      "tem 0.16405265145195277\n",
      "semest 0.15247253234916563\n",
      "microscopi 0.1516874012613179\n",
      "project 0.15051638466790326\n",
      "----------------- TOPIC 8 ---------------------\n",
      "ee 0.14850215063443672\n",
      "('circuitsinterpret', 'transistor-level') 0.1467776001868965\n",
      "('synthet', 'effici') 0.14462961420168502\n",
      "displac 0.14370170982649144\n",
      "('design', 'key') 0.14047441456552015\n",
      "solar 0.13992229797443712\n",
      "('background', 'synthet') 0.13957936567004534\n",
      "flow 0.13853444742693946\n",
      "ethic 0.13778268193870027\n",
      "expertis 0.13681367162843194\n",
      "----------------- TOPIC 9 ---------------------\n",
      "optic 0.1577941576307777\n",
      "control 0.15400830800056628\n",
      "system 0.15362997357678773\n",
      "properti 0.15189729836326507\n",
      "pharmacolog 0.1516080489362889\n",
      "signal 0.14839035236824039\n",
      "servic 0.14780120200904154\n",
      "measur 0.14736402324131856\n",
      "epistemolog 0.14566609097750527\n",
      "spectroscopi 0.14259549978065553\n",
      "----------------- TOPIC 10 ---------------------\n",
      "('question', 'chemic') 0.29218745189386053\n",
      "('provid', 'fondament') 0.29069487802997235\n",
      "('fondament', 'question') 0.26698836806316595\n",
      "('compound', 'drug') 0.26623870996001303\n",
      "('chemic', 'compound') 0.2638397610736499\n",
      "('drug', '?') 0.2631383541739458\n",
      "fondament 0.24990721850113318\n",
      "drug 0.21849314407535497\n",
      "compound 0.19710863095358253\n",
      "? 0.18045979726079686\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_f = csr_matrix.toarray(X)\n",
    "\n",
    "mX = np.zeros(87304)\n",
    "for i in range(87304):\n",
    "    mX[i] = np.max(X_f[i,:]) # maximum TF-IDF for each term -> it tells us importance of that term.\n",
    "\n",
    "indx_map = mX.argsort()[-10000:][::-1]\n",
    "X_f = X_f[indx_map,:]\n",
    "\n",
    "data = []\n",
    "for i in range(854):\n",
    "    data.append([i+1, Vectors.dense(X_f[:,i])])\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = LDA.train(rdd, k = 10, maxIterations = 20, optimizer = 'online', docConcentration = 0.01, topicConcentration = 0.01, seed = 1)\n",
    "\n",
    "topics = model.topicsMatrix()\n",
    "explain_topics(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out several values for $k$, 10 is the one that made more sense based on the topics yielded. The values of the other 2 hyperparameters were decided in the previous exercise based on how they affected the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels for the 10 topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training rotation, doctorate. (strong)\n",
    "2. -. (weak)\n",
    "3. Lifecycle, magnets, geology. (weak)\n",
    "4. Programming, networks, etc.\n",
    "5. EDMT management of technology. (strong)\n",
    "6. Biomass, optics, cells, molecules, etc. (normal)\n",
    "7. IC projects. (strong)\n",
    "8. EE, transistors, etc. (weak)\n",
    "9. Optics, control, and signal. (normal)\n",
    "10. Chemistry, drugs, and compunds. (strong)\n",
    "\n",
    "Strong, normal, or weak stand for the weight of the words related to the topic. Hence it means how much evidence we have for syaing this is the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Extract the structure in terms of topics from the wikipedia-for-school dataset. Use your\n",
    "intuition about how many topics might be covered by the articles and how they are distributed.\n",
    "\n",
    "1. Report the values for k, α and β that you chose a priori and why you picked them.\n",
    "2. Are you convinced by the results? Give labels to the topics if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = sc.textFile(\"/ix/wikipedia-for-schools.txt\").map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5554 documents in total.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", wiki.count() , \"documents in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'courseId': 'MGT-430',\n",
       " 'name': 'Quantitative systems modeling techniques',\n",
       " 'description': ['dedic',\n",
       "  'model',\n",
       "  'tool',\n",
       "  'optim',\n",
       "  'method',\n",
       "  'decis',\n",
       "  'analysi',\n",
       "  'techniqu',\n",
       "  'specif',\n",
       "  'focu',\n",
       "  'logist',\n",
       "  'content',\n",
       "  'introduct',\n",
       "  'oper',\n",
       "  'research',\n",
       "  'graph',\n",
       "  'color',\n",
       "  'linear',\n",
       "  'program',\n",
       "  'flow',\n",
       "  'theori',\n",
       "  'graph',\n",
       "  'cover',\n",
       "  'model',\n",
       "  'applic',\n",
       "  'network',\n",
       "  'design',\n",
       "  'distribut',\n",
       "  'transport',\n",
       "  'distribut',\n",
       "  'heurist',\n",
       "  'method',\n",
       "  'vehicl',\n",
       "  'rout',\n",
       "  'problem',\n",
       "  'facil',\n",
       "  'locat',\n",
       "  'problem',\n",
       "  'job',\n",
       "  'shop',\n",
       "  'facil',\n",
       "  'layout',\n",
       "  'balanc',\n",
       "  'assembl',\n",
       "  'line',\n",
       "  'open',\n",
       "  'shop',\n",
       "  'keyword',\n",
       "  'model',\n",
       "  'techniqu',\n",
       "  'oper',\n",
       "  'research',\n",
       "  'learn',\n",
       "  'outcom',\n",
       "  'end',\n",
       "  'student',\n",
       "  'repres',\n",
       "  'import',\n",
       "  'logist',\n",
       "  'problem',\n",
       "  'oper',\n",
       "  'research',\n",
       "  'models.solv',\n",
       "  'problem',\n",
       "  'exact',\n",
       "  'method',\n",
       "  'heuristics.classifi',\n",
       "  'optim',\n",
       "  'problem',\n",
       "  'transvers',\n",
       "  'skill',\n",
       "  'summar',\n",
       "  'articl',\n",
       "  'technic',\n",
       "  'report.access',\n",
       "  'evalu',\n",
       "  'sourc',\n",
       "  'inform',\n",
       "  'teach',\n",
       "  'method',\n",
       "  'lectur',\n",
       "  'theoret',\n",
       "  'part',\n",
       "  'exercis',\n",
       "  'expect',\n",
       "  'student',\n",
       "  'activ',\n",
       "  'attend',\n",
       "  'lectur',\n",
       "  'complet',\n",
       "  'exercis',\n",
       "  'assess',\n",
       "  'method',\n",
       "  'written',\n",
       "  'exam',\n",
       "  'document',\n",
       "  'allow',\n",
       "  'mid-term',\n",
       "  'exam',\n",
       "  'final',\n",
       "  'exam',\n",
       "  'supervis',\n",
       "  'offic',\n",
       "  'hour',\n",
       "  'assist',\n",
       "  'forum',\n",
       "  'firstli',\n",
       "  'contact',\n",
       "  'e-mail',\n",
       "  ('dedic', 'model'),\n",
       "  ('model', 'tool'),\n",
       "  ('tool', 'optim'),\n",
       "  ('optim', 'method'),\n",
       "  ('method', 'decis'),\n",
       "  ('decis', 'analysi'),\n",
       "  ('analysi', 'techniqu'),\n",
       "  ('techniqu', 'specif'),\n",
       "  ('specif', 'focu'),\n",
       "  ('focu', 'logist'),\n",
       "  ('content', 'introduct'),\n",
       "  ('introduct', 'oper'),\n",
       "  ('oper', 'research'),\n",
       "  ('research', 'graph'),\n",
       "  ('graph', 'color'),\n",
       "  ('color', 'linear'),\n",
       "  ('linear', 'program'),\n",
       "  ('program', 'flow'),\n",
       "  ('flow', 'theori'),\n",
       "  ('theori', 'graph'),\n",
       "  ('graph', 'cover'),\n",
       "  ('cover', 'model'),\n",
       "  ('model', 'applic'),\n",
       "  ('applic', 'network'),\n",
       "  ('network', 'design'),\n",
       "  ('design', 'distribut'),\n",
       "  ('distribut', 'transport'),\n",
       "  ('transport', 'distribut'),\n",
       "  ('distribut', 'heurist'),\n",
       "  ('heurist', 'method'),\n",
       "  ('method', 'vehicl'),\n",
       "  ('vehicl', 'rout'),\n",
       "  ('rout', 'problem'),\n",
       "  ('problem', 'facil'),\n",
       "  ('facil', 'locat'),\n",
       "  ('locat', 'problem'),\n",
       "  ('problem', 'job'),\n",
       "  ('job', 'shop'),\n",
       "  ('shop', 'facil'),\n",
       "  ('facil', 'layout'),\n",
       "  ('layout', 'balanc'),\n",
       "  ('balanc', 'assembl'),\n",
       "  ('assembl', 'line'),\n",
       "  ('line', 'open'),\n",
       "  ('open', 'shop'),\n",
       "  ('keyword', 'model'),\n",
       "  ('model', 'techniqu'),\n",
       "  ('techniqu', 'oper'),\n",
       "  ('oper', 'research'),\n",
       "  ('research', 'learn'),\n",
       "  ('learn', 'outcom'),\n",
       "  ('outcom', 'end'),\n",
       "  ('end', 'student'),\n",
       "  ('student', 'repres'),\n",
       "  ('repres', 'import'),\n",
       "  ('import', 'logist'),\n",
       "  ('logist', 'problem'),\n",
       "  ('problem', 'oper'),\n",
       "  ('oper', 'research'),\n",
       "  ('research', 'model'),\n",
       "  ('solv', 'problem'),\n",
       "  ('problem', 'exact'),\n",
       "  ('exact', 'method'),\n",
       "  ('method', 'heurist'),\n",
       "  ('classifi', 'optim'),\n",
       "  ('optim', 'problem'),\n",
       "  ('problem', 'transvers'),\n",
       "  ('transvers', 'skill'),\n",
       "  ('skill', 'summar'),\n",
       "  ('summar', 'articl'),\n",
       "  ('articl', 'technic'),\n",
       "  ('technic', 'report'),\n",
       "  ('access', 'evalu'),\n",
       "  ('evalu', 'sourc'),\n",
       "  ('sourc', 'inform'),\n",
       "  ('teach', 'method'),\n",
       "  ('method', 'lectur'),\n",
       "  ('lectur', 'theoret'),\n",
       "  ('theoret', 'part'),\n",
       "  ('part', 'exercis'),\n",
       "  ('exercis', 'expect'),\n",
       "  ('expect', 'student'),\n",
       "  ('student', 'activ'),\n",
       "  ('activ', 'attend'),\n",
       "  ('attend', 'lectur'),\n",
       "  ('lectur', 'complet'),\n",
       "  ('complet', 'exercis'),\n",
       "  ('exercis', 'assess'),\n",
       "  ('assess', 'method'),\n",
       "  ('method', 'written'),\n",
       "  ('written', 'exam'),\n",
       "  ('exam', 'document'),\n",
       "  ('document', 'allow'),\n",
       "  ('allow', 'mid-term'),\n",
       "  ('mid-term', 'exam'),\n",
       "  ('exam', 'final'),\n",
       "  ('final', 'exam'),\n",
       "  ('exam', 'supervis'),\n",
       "  ('supervis', 'offic'),\n",
       "  ('offic', 'hour'),\n",
       "  ('hour', 'assist'),\n",
       "  ('assist', 'forum'),\n",
       "  ('forum', 'firstli'),\n",
       "  ('firstli', 'contact'),\n",
       "  ('contact', 'e-mail')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_stem_corpus[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikin = np.asarray(wiki.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikin is a list in which every element (5554 in total) is a dictionary with:\n",
    "* pageID\n",
    "* title\n",
    "* tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(d):\n",
    "    global_dictionary = []\n",
    "    dictionary_mapping = {}\n",
    "    for i in range(0,len(d)):\n",
    "        temp_list = d[i]\n",
    "        global_dictionary = global_dictionary + temp_list['tokens']\n",
    "        for w in temp_list['tokens']:\n",
    "            if w in dictionary_mapping:\n",
    "                dictionary_mapping[w].append(i)\n",
    "            else:\n",
    "                dictionary_mapping[w] = [i]\n",
    "    return global_dictionary, dictionary_mapping\n",
    "\n",
    "def get_term_document_matrix(corpus):\n",
    "    global_dictionary, dictionary_mapping = get_dictionary(corpus)\n",
    "    df_index = dict((k,len(list(set(v)))) for k,v in dictionary_mapping.items()) # A dict where the key is the term and the value is in how many documents the term is present\n",
    "    unique_words = list(df_index.keys()) #The unique words\n",
    "    word_to_index = dict(zip(unique_words,(range(len(unique_words))))) # Mapping from word to index (That we use for the encoding)\n",
    "    index_to_word = dict((v,k) for k,v in word_to_index.items()) # Mapping back from index to word. \n",
    "\n",
    "\n",
    "    m = len(unique_words)\n",
    "    n = len(corpus)\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    columns = []\n",
    "\n",
    "    for i in range(n):\n",
    "        tokens = corpus[i]['tokens']\n",
    "        loc_word_count = len(tokens)\n",
    "        loc_counts = Counter(tokens)\n",
    "        unique_tokens = list(loc_counts.keys())\n",
    "\n",
    "        for token in unique_tokens:\n",
    "            tf = loc_counts[token]/loc_word_count\n",
    "            df = df_index[token]\n",
    "            idf = np.log(n/(df+1))\n",
    "\n",
    "            rows.append(word_to_index[token])\n",
    "            columns.append(i)\n",
    "            values.append(tf*idf)\n",
    "\n",
    "    return csr_matrix((values, (rows, columns)), shape=(m, n)), index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw, iww, wiw = get_term_document_matrix(wikin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = iww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw_f = csr_matrix.toarray(Xw)\n",
    "\n",
    "mX = np.zeros(87304)\n",
    "for i in range(87304):\n",
    "    mX[i] = np.max(Xw_f[i,:]) # maximum TF-IDF for each term -> it tells us importance of that term.\n",
    "\n",
    "indx_map = mX.argsort()[-10000:][::-1]\n",
    "Xw_f = Xw_f[indx_map,:]\n",
    "\n",
    "data = []\n",
    "for i in range(854):\n",
    "    data.append([i+1, Vectors.dense(Xw_f[:,i])])\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "model = LDA.train(rdd, k = 20, maxIterations = 20, optimizer = 'online', docConcentration = 0.01, topicConcentration = 0.01, seed = 1)\n",
    "\n",
    "topics = model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explain_topics(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking $k = 20$ the topics make more sense than with other $k$'s I have chosen before.\n",
    "\n",
    "2. Animals and plants\n",
    "6. Amazon\n",
    "\n",
    "However, I am still not convinced and find it difficult to find it interpretable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
